"""
============================================
INTEGRACI√ìN COMPLETA DEL SISTEMA RAG
============================================
Este archivo integra todos los componentes:
1. Pipeline de Recuperaci√≥n
2. LLM Generator
3. Sistema Conversacional con Memoria

USO EN EL NOTEBOOK:
Ejecuta este c√≥digo despu√©s de tener definidos:
- hybrid_search (b√∫squeda vectorial)
- table_searcher (b√∫squeda tabular)
- graph_searcher (b√∫squeda en grafos)
- KeywordClassifier (clasificador)
"""

# ==================== IMPORTS ====================
# (Aseg√∫rate de haber ejecutado las celdas anteriores con las clases necesarias)

from typing import List, Dict

# ==================== CONFIGURACI√ìN ====================

# Primero, aseg√∫rate de tener tu configuraci√≥n
class Config:
    """Configuraci√≥n global - Ajusta seg√∫n tu caso"""
    
    # Elige tu proveedor de LLM
    LLM_PROVIDER = "gemini"  # Opciones: 'gemini', 'openai', 'groq', 'ollama'
    
    # API Keys (comenta las que no uses)
    GEMINI_API_KEY = "tu-api-key-aqui"      # https://aistudio.google.com/apikey
    # OPENAI_API_KEY = "sk-..."             # https://platform.openai.com/api-keys
    # GROQ_API_KEY = "gsk_..."              # https://console.groq.com/keys
    
    # Par√°metros del LLM
    LLM_MODEL = "gemini-2.5-flash"  # o "gpt-4o-mini", "llama-3.3-70b-versatile"
    LLM_TEMPERATURE = 0.7
    LLM_MAX_TOKENS = 1024
    
    # Par√°metros de recuperaci√≥n
    RETRIEVAL_TOP_K = 5
    MAX_CONVERSATION_HISTORY = 10

config = Config()

# ==================== PASO 1: CREAR PIPELINE ====================

print("="*60)
print("üîß INICIALIZANDO SISTEMA RAG")
print("="*60 + "\n")

print("1Ô∏è‚É£ Creando Pipeline de Recuperaci√≥n...")

# Crear clasificador keyword (backup si el LLM no funciona)
keyword_classifier = KeywordClassifier()

# Crear pipeline de recuperaci√≥n
pipeline = RetrievalPipeline(
    vector_search=hybrid_search,      # Ya debe estar definido
    table_search=table_searcher,      # Ya debe estar definido
    graph_search=graph_searcher,      # Ya debe estar definido
    classifier=keyword_classifier
)

print("‚úÖ Pipeline creado\n")

# ==================== PASO 2: CREAR LLM GENERATOR ====================

print("2Ô∏è‚É£ Inicializando LLM Generator...")
print(f"   Proveedor: {config.LLM_PROVIDER}")
print(f"   Modelo: {config.LLM_MODEL}")

try:
    if config.LLM_PROVIDER == "gemini":
        llm = create_llm_generator(
            provider='gemini',
            api_key=config.GEMINI_API_KEY,
            model=config.LLM_MODEL,
            temperature=config.LLM_TEMPERATURE,
            max_tokens=config.LLM_MAX_TOKENS
        )
    
    elif config.LLM_PROVIDER == "openai":
        llm = create_llm_generator(
            provider='openai',
            api_key=config.OPENAI_API_KEY,
            model=config.LLM_MODEL,
            temperature=config.LLM_TEMPERATURE,
            max_tokens=config.LLM_MAX_TOKENS
        )
    
    elif config.LLM_PROVIDER == "groq":
        llm = create_llm_generator(
            provider='groq',
            api_key=config.GROQ_API_KEY,
            model=config.LLM_MODEL,
            temperature=config.LLM_TEMPERATURE,
            max_tokens=config.LLM_MAX_TOKENS
        )
    
    elif config.LLM_PROVIDER == "ollama":
        llm = create_llm_generator(
            provider='ollama',
            model=config.LLM_MODEL,
            temperature=config.LLM_TEMPERATURE
        )
    
    else:
        raise ValueError(f"Proveedor no soportado: {config.LLM_PROVIDER}")
    
    print("‚úÖ LLM Generator inicializado\n")

except Exception as e:
    print(f"‚ùå Error al inicializar LLM: {e}")
    print("üí° Verifica tu API key y configuraci√≥n")
    raise

# ==================== PASO 3: CREAR SISTEMA CONVERSACIONAL ====================

print("3Ô∏è‚É£ Creando Sistema Conversacional...")

rag_system = ConversationalRAG(
    retrieval_pipeline=pipeline,
    llm_generator=llm,
    memory=ConversationMemory(max_history=config.MAX_CONVERSATION_HISTORY),
    language='es'
)

print("‚úÖ Sistema RAG completo inicializado\n")
print("="*60)
print("üéâ SISTEMA LISTO PARA USAR")
print("="*60 + "\n")

# ==================== PRUEBAS R√ÅPIDAS ====================

print("üß™ EJECUTANDO PRUEBAS R√ÅPIDAS...\n")

# Definir consultas de prueba
test_queries = [
    "¬øC√≥mo uso mi licuadora para hacer smoothies?",              # Vectorial
    "¬øCu√°les son las licuadoras de menos de $200?",             # Tabular
    "¬øQu√© productos son compatibles con la batidora P0005?",    # Grafo
    "¬øQu√© opinan los usuarios de las licuadoras TechHome?",     # Vectorial
    "Muestra productos con garant√≠a mayor a 24 meses"           # Tabular
]

# Ejecutar pruebas
results = batch_test(rag_system, test_queries)

# ==================== MODO INTERACTIVO ====================

print("\n" + "="*60)
print("üí¨ MODO INTERACTIVO")
print("="*60)
print("Para iniciar el chat interactivo, ejecuta:")
print(">>> interactive_chat(rag_system)")
print("\nO prueba consultas individuales:")
print(">>> result = rag_system.chat('¬øC√≥mo funciona mi licuadora?')")
print(">>> print(result['response'])")
print("="*60 + "\n")

# ==================== EJEMPLOS DE USO ====================

def ejemplo_consulta_simple():
    """Ejemplo de consulta simple"""
    print("\n" + "="*60)
    print("EJEMPLO 1: Consulta Simple")
    print("="*60 + "\n")
    
    result = rag_system.chat("¬øC√≥mo limpio mi licuadora?")
    
    print(f"Consulta: {result['query']}")
    print(f"Respuesta: {result['response']}")
    print(f"Fuente: {result['source']}")
    print(f"Documentos recuperados: {result['retrieval_count']}")


def ejemplo_conversacion():
    """Ejemplo de conversaci√≥n con contexto"""
    print("\n" + "="*60)
    print("EJEMPLO 2: Conversaci√≥n con Contexto")
    print("="*60 + "\n")
    
    # Resetear conversaci√≥n
    rag_system.reset_conversation()
    
    # Primera consulta
    r1 = rag_system.chat("¬øQu√© licuadoras tienes disponibles?")
    print(f"üßë Usuario: ¬øQu√© licuadoras tienes disponibles?")
    print(f"ü§ñ Asistente: {r1['response'][:200]}...\n")
    
    # Segunda consulta (con contexto)
    r2 = rag_system.chat("¬øCu√°l me recomiendas para hacer smoothies?")
    print(f"üßë Usuario: ¬øCu√°l me recomiendas para hacer smoothies?")
    print(f"ü§ñ Asistente: {r2['response'][:200]}...\n")
    
    # Tercera consulta
    r3 = rag_system.chat("¬øCu√°nto cuesta?")
    print(f"üßë Usuario: ¬øCu√°nto cuesta?")
    print(f"ü§ñ Asistente: {r3['response'][:200]}...\n")


def ejemplo_fuente_especifica():
    """Ejemplo especificando la fuente de datos"""
    print("\n" + "="*60)
    print("EJEMPLO 3: Especificar Fuente de Datos")
    print("="*60 + "\n")
    
    # Forzar b√∫squeda vectorial
    r1 = rag_system.chat(
        "informaci√≥n sobre licuadoras",
        source='vectorial',
        top_k=3
    )
    print(f"B√∫squeda vectorial: {r1['response'][:150]}...")
    
    # Forzar b√∫squeda tabular
    r2 = rag_system.chat(
        "informaci√≥n sobre licuadoras",
        source='tabular'
    )
    print(f"B√∫squeda tabular: {r2['response'][:150]}...")


# ==================== EJECUTAR EJEMPLOS ====================

# Descomenta para ejecutar ejemplos:
# ejemplo_consulta_simple()
# ejemplo_conversacion()
# ejemplo_fuente_especifica()

# ==================== ESTAD√çSTICAS FINALES ====================

def mostrar_estadisticas():
    """Muestra estad√≠sticas del sistema"""
    print("\n" + "="*60)
    print("üìä ESTAD√çSTICAS DEL SISTEMA")
    print("="*60)
    
    print(rag_system.get_conversation_summary())
    pipeline.print_stats()
    llm.print_stats()

# Descomentar para ver estad√≠sticas:
# mostrar_estadisticas()

# ==================== NOTAS IMPORTANTES ====================

"""
NOTAS DE USO:

1. CONFIGURACI√ìN INICIAL:
   - Ajusta Config() con tu proveedor de LLM preferido
   - Aseg√∫rate de tener las API keys necesarias
   - Verifica que todos los componentes previos est√©n definidos

2. PROVEEDORES DE LLM RECOMENDADOS:
   
   a) Gemini (Google):
      - Gratuito con l√≠mites
      - API: https://aistudio.google.com/apikey
      - Modelo: gemini-2.5-flash
      - Nota: Puede tener filtros de seguridad restrictivos
   
   b) OpenAI:
      - Requiere cr√©ditos ($5 m√≠nimo)
      - API: https://platform.openai.com/api-keys
      - Modelo: gpt-4o-mini (m√°s barato y r√°pido)
      - Muy confiable, excelente calidad
   
   c) Groq:
      - GRATIS con l√≠mites generosos
      - API: https://console.groq.com/keys
      - Modelo: llama-3.3-70b-versatile
      - MUY R√ÅPIDO, excelente opci√≥n gratuita
   
   d) Ollama:
      - Completamente local
      - Requiere instalaci√≥n: https://ollama.ai
      - Modelo: llama3, mistral, etc.
      - Sin costos, sin l√≠mites, privado

3. RECOMENDACI√ìN PARA EL TP:
   
   Si Gemini no funciona (filtros de seguridad), usa GROQ:
   - Es gratuito
   - Sin filtros restrictivos
   - Muy r√°pido
   - Excelente calidad de respuestas
   
   C√≥digo para cambiar a Groq:
   ```python
   config.LLM_PROVIDER = "groq"
   config.GROQ_API_KEY = "gsk_..."  # Tu API key de Groq
   config.LLM_MODEL = "llama-3.3-70b-versatile"
   ```

4. PARA EL INFORME:
   
   Menciona:
   - Proveedor de LLM usado y justificaci√≥n
   - Modelo espec√≠fico
   - Configuraci√≥n (temperatura, max_tokens)
   - Resultados de las pruebas
   - Limitaciones encontradas

5. TROUBLESHOOTING:
   
   - Error de API key: Verifica que est√© bien copiada
   - Respuestas bloqueadas: Cambia de proveedor
   - Errores de rate limit: Espera o cambia de proveedor
   - Respuestas en ingl√©s: Verifica language='es' en ConversationalRAG
"""
